{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cwhitz/ts-trove/blob/master/notebooks/classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVkS8EnJ6hfS"
      },
      "source": [
        "# Time Series Classification\n",
        "\n",
        "This notebook explores various time series classification techniques. It makes much fuller use of the bearings dataset also explored in the signal analysis notebook.\n",
        "\n",
        "## Overview\n",
        "\n",
        "Time series classification involves assigning time series instances to predefined categories. This notebook will cover:\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "> 1 [Data Preparation](#Data-Preparation)\n",
        "\n",
        "1.1. [Data Download](##Data-Download)\n",
        "\n",
        "1.2 [Data Organization](##Data-Organization)\n",
        "\n",
        "> 2 [Utility Functions](#Training-Functions)\n",
        "\n",
        "2.1 [Data Loader](##Data-Loader)\n",
        "\n",
        "> 3 [Time Series Classification with SciKit](#Scikit-Functions)\n",
        "\n",
        "3.1 [Scikit Trainer and Evaluator](##Scikit-Trainer-and-Evaluator)\n",
        "\n",
        "> 4 [Deep Learning Models](#Deep-Learning-Models)\n",
        "\n",
        "4.1 [PyTorch Trainer and Evaluator](##-Trainer-and-Evaluator)\n",
        "\n",
        "4.2 [Fully Connected Neural Networks]()\n",
        "\n",
        "4.3 [Recurrent Neural Networks]()\n",
        "\n",
        "4.3.1 [Classic Recurrent Neural Network]()\n",
        "\n",
        "4.3.2 [Long Short Term Memory (LSTM) Neural Network]()\n",
        "\n",
        "4.3.3 [Gated Recurrent Neural Network]()\n",
        "\n",
        "4.4 [Convolutional Neural Networks]()\n",
        "\n",
        "4.4.1 [1D Convolutional Neural Network]()\n",
        "\n",
        "4.4.2 [Temporal Convolutional Network]()\n",
        "\n",
        "4.5 [Attention Based Models]()\n",
        "\n",
        "4.5.1 [LSTM with Attention]()\n",
        "\n",
        "4.5.2 [Time Series Transformer]()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MKXciZyo6hfT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import pathlib\n",
        "import shutil\n",
        "import kagglehub\n",
        "\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cesium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cs3qdujLVjZK",
        "outputId": "931f985d-544c-478a-df6d-5caa47146a93"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cesium in /usr/local/lib/python3.12/dist-packages (0.12.4)\n",
            "Requirement already satisfied: numpy<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from cesium) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from cesium) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.12/dist-packages (from cesium) (1.6.1)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.12/dist-packages (from cesium) (2.2.2)\n",
            "Requirement already satisfied: dask>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from cesium) (2025.12.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.12/dist-packages (from cesium) (0.12.1)\n",
            "Requirement already satisfied: gatspy>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from cesium) (0.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from cesium) (3.1.2)\n",
            "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.12/dist-packages (from cesium) (1.5.3)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.12/dist-packages (from dask>=2.5.0->cesium) (8.3.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.12/dist-packages (from dask>=2.5.0->cesium) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from dask>=2.5.0->cesium) (25.0)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from dask>=2.5.0->cesium) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from dask>=2.5.0->cesium) (6.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.17.0->cesium) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.17.0->cesium) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.17.0->cesium) (2025.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.1->cesium) (3.6.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.12/dist-packages (from partd>=1.4.0->dask>=2.5.0->cesium) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.17.0->cesium) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n",
        "\n",
        "## Data Overview\n",
        "\n",
        "**What is this dataset?**\n",
        "\n",
        "This is a collection of vibration data from electric motor bearings. Bearings those small spinning parts that let machinery rotate smoothly. Think of a bearing like the axle in a wheel: it's got little metal balls inside that roll around, letting a shaft spin with barely any friction.\n",
        "\n",
        "**Why was this dataset created?**\n",
        "\n",
        "The researchers at Case Western Reserve University deliberately damaged bearings in different ways, then recorded how the motor vibrated as a result. They made tiny cracks of various sizes (ranging from 7 to 40 thousandths of an inch) in the bearings, then attached vibration sensors to measure what happened.\n",
        "\n",
        "Cracks of different mm sizes were introduced on the outer race, inner race, and the balls themselves.\n",
        "\n",
        "![ball_bearing_diagram](https://www.globalspec.com/ImageRepository/LearnMore/20133/ball%20bearing5364b00280ef4db7b85dfba113f04556.png)\n",
        "\n",
        "The goal was to understand the relationship between bearing damage and vibration patterns, creating a reference library that shows what different types of bearing failure look like.\n",
        "\n",
        "**Why is it useful?**\n",
        "\n",
        "This data is incredibly useful for real-world maintenance and diagnostics. In factories and power plants, you can use vibration patterns to detect bearing problems before they cause catastrophic failures. By comparing vibrations from a running machine to patterns in this dataset, maintenance teams can identify early signs of wear, predict when a bearing will fail, and schedule repairs before expensive downtime happens. It's basically like a fingerprint database for bearing damage—once you know what a damaged bearing \"sounds like,\" you can spot trouble coming.\n",
        "\n",
        "**So what are we actually trying to predict?**\n",
        "\n",
        "Good question. We will try to train machine learning models to predict three things: 1) Is the bearing in normal operation? 2) If not, where is the crack? 3) And what size is it?\n",
        "\n",
        "2 and 3 of course become irrelevant if the bearing is in normal operation, but they allow us to go a step beyond simple detection of irregular operation.\n",
        "\n",
        "\n",
        "## Data Download\n",
        "\n",
        "The raw data can be downloaded directly from Kaggle."
      ],
      "metadata": {
        "id": "jlDAmb6rUOA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kagglepath = \"sufian79/cwru-mat-full-dataset\"\n",
        "path = kagglehub.dataset_download(kagglepath)\n",
        "\n",
        "\n",
        "pathlib.Path(f\"./{kagglepath.split('/')[-1]}\").mkdir(parents=True, exist_ok=True)\n",
        "shutil.copytree(path, f\"./{kagglepath.split('/')[-1]}\", dirs_exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "ak_iKkQdUCfF",
        "outputId": "e69b0490-18dc-4c06-fea1-8db053c545d1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'cwru-mat-full-dataset' dataset.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./cwru-mat-full-dataset'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Organization\n",
        "\n",
        "The raw data is a collection of numbered mat files and requires reference back to the [original website](https://engineering.case.edu/bearingdatacenter/48k-drive-end-bearing-fault-data) to make sense of. I've gone ahead and done that with the JSON structure below.\n",
        "\n",
        "The data is organized at top-level describing the type of fault, or lack thereof with \"normal\" sample files are the motor operating without faults. The next level down is the sampling rate, followed by the location where the crack was introduced (IR being inner race, B being ball, OR being outer race) and then finally, the size of the cracks ranging from 7 to 21 mm.\n",
        "\n",
        "The code below this cell moves the individual samples into folders matching the structure below, which aligns with how PyTorch's DataSet and DataLoader work (we will make it work for scikit too)."
      ],
      "metadata": {
        "id": "ZvtGS34aluAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_structure = {\n",
        "  \"normal\": {\n",
        "    \"48k\": [\"97\", \"98\", \"99\", \"100\"]\n",
        "  },\n",
        "  \"drive_end_fault\": {\n",
        "    \"12k\": {\n",
        "      \"IR\": {\n",
        "        \"007\": [\"105\", \"106\", \"107\", \"108\"],\n",
        "        \"014\": [\"169\", \"170\", \"171\", \"172\"],\n",
        "        \"021\": [\"209\", \"210\", \"211\", \"212\"]\n",
        "      },\n",
        "\n",
        "      \"B\": {\n",
        "        \"007\": [\"118\", \"119\", \"120\", \"121\"],\n",
        "        \"014\": [\"185\", \"186\", \"187\", \"188\"],\n",
        "        \"021\": [\"222\", \"223\", \"224\", \"225\"]\n",
        "      },\n",
        "\n",
        "      \"OR\": {\n",
        "        \"007\": [\"130\", \"131\", \"132\", \"133\"],\n",
        "        \"014\": [\"197\", \"198\", \"199\", \"200\"],\n",
        "        \"021\": [\"234\", \"235\", \"236\", \"237\"]\n",
        "      }\n",
        "    },\n",
        "\n",
        "    \"48k\": {\n",
        "      \"IR\": {\n",
        "        \"007\": [\"109\", \"110\", \"111\", \"112\"],\n",
        "        \"014\": [\"174\", \"175\", \"176\", \"177\"],\n",
        "        \"021\": [\"213\", \"214\", \"215\", \"217\"]\n",
        "      },\n",
        "\n",
        "      \"B\": {\n",
        "        \"007\": [\"122\", \"123\", \"124\", \"125\"],\n",
        "        \"014\": [\"189\", \"190\", \"191\", \"192\"],\n",
        "        \"021\": [\"226\", \"227\", \"228\", \"229\"]\n",
        "      },\n",
        "\n",
        "      \"OR\": {\n",
        "        \"007\": [\"135\", \"136\", \"137\", \"138\"],\n",
        "        \"014\": [\"201\", \"202\", \"203\", \"204\"],\n",
        "        \"021\": [\"238\", \"239\", \"240\", \"241\"]\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "\n",
        "  \"fan_end_fault\": {\n",
        "    \"12k\": {\n",
        "      \"IR\": {\n",
        "        \"007\": [\"278\", \"279\", \"280\", \"281\"],\n",
        "        \"014\": [\"274\", \"275\", \"276\", \"277\"],\n",
        "        \"021\": [\"270\", \"271\", \"272\", \"273\"]\n",
        "      },\n",
        "\n",
        "      \"B\": {\n",
        "        \"007\": [\"282\", \"283\", \"284\", \"285\"],\n",
        "        \"014\": [\"286\", \"287\", \"288\", \"289\"],\n",
        "        \"021\": [\"290\", \"291\", \"292\", \"293\"]\n",
        "      },\n",
        "\n",
        "      \"OR\": {\n",
        "        \"007\": [\"298\", \"299\", \"300\", \"301\"],\n",
        "        \"014\": [\"309\", \"310\", \"311\", \"312\"],\n",
        "        \"021\": [\"315\", \"316\", \"317\", \"318\"]\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "fe1oXdcK2h1E"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_DIR = \"cwru-mat-full-dataset/\"\n",
        "TARGET_DIR = \"classification-cwru-mat-organized\"\n",
        "FILE_EXTENSION = \".mat\"\n",
        "\n",
        "def ensure_dir(path):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def move_file(file_id, dest_dir):\n",
        "    filename = file_id + FILE_EXTENSION\n",
        "    src_path = os.path.join(SOURCE_DIR, filename)\n",
        "    dst_path = os.path.join(dest_dir, filename)\n",
        "\n",
        "    if not os.path.exists(src_path):\n",
        "        print(f\"⚠️ Missing file: {src_path}\")\n",
        "        return\n",
        "\n",
        "    ensure_dir(dest_dir)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "def walk_structure(node, current_path):\n",
        "    if isinstance(node, list):\n",
        "        for file_id in node:\n",
        "            move_file(file_id, current_path)\n",
        "    elif isinstance(node, dict):\n",
        "        for key, child in node.items():\n",
        "            walk_structure(child, os.path.join(current_path, key))\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected structure type\")\n",
        "\n",
        "\n",
        "walk_structure(folder_structure, TARGET_DIR)\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUY9UIluUtx-",
        "outputId": "53b86345-1894-43da-ad18-e6054dc0680d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59mefCkU6hfU"
      },
      "source": [
        "# Utility Functions\n",
        "\n",
        "## Data Loader\n",
        "\n",
        "Before diving into modeling, we first need a consistent way to load and represent our time-series data. Since later sections will experiment with both deep learning and traditional classifiers, we define a reusable dataset structure that keeps preprocessing, sampling rate handling, and labels consistent across all methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "N9S8fo-16hfU"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.nn import Module\n",
        "import scipy.io\n",
        "import enum\n",
        "\n",
        "# samplng rate enum\n",
        "class SamplingRate(enum.Enum):\n",
        "    sr12K = \"12k\"\n",
        "    sr48K = \"48k\"\n",
        "\n",
        "class FaultLocation(enum.Enum):\n",
        "    DE = \"drive_end_fault\"\n",
        "    FE = \"front_end_fault\"\n",
        "\n",
        "\n",
        "class BearingDataset(Dataset):\n",
        "    def __init__(self, file_paths, sampling_rate, fault_location, chunk_length, unified_label=True, transform=None):\n",
        "        self.file_paths = file_paths\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.fault_location = fault_location\n",
        "        self.chunk_length = chunk_length\n",
        "        self.transform = transform\n",
        "        self.unified_label = unified_label\n",
        "\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "\n",
        "        self._organize_data()\n",
        "\n",
        "    def _organize_data(self):\n",
        "        for fp in self.file_paths:\n",
        "            if not pathlib.Path(fp).exists():\n",
        "                raise FileNotFoundError(f\"File not found: {fp}\")\n",
        "\n",
        "            mat_data = scipy.io.loadmat(fp)\n",
        "\n",
        "            key_to_match = f\"_{str(self.fault_location)[-2:]}_time\"\n",
        "            sensor_key = [key for key in mat_data.keys() if key_to_match in key][0]\n",
        "\n",
        "            signal = mat_data[sensor_key].squeeze()\n",
        "\n",
        "            n_chunks = len(signal) // self.chunk_length\n",
        "            truncated = signal[:n_chunks * self.chunk_length]\n",
        "\n",
        "            windows = truncated.reshape(n_chunks, self.chunk_length)\n",
        "\n",
        "            label_parts = fp.parent.parts\n",
        "\n",
        "            if label_parts[0] == 'normal':\n",
        "                label_dict = {\n",
        "                    'normal': True,\n",
        "                    'fault_location': None,\n",
        "                    'crack_size': None\n",
        "                }\n",
        "            else:\n",
        "                label_dict = {\n",
        "                    'normal': False,\n",
        "                    'fault_location': label_parts[-2],\n",
        "                    'crack_size': label_parts[-1]\n",
        "                }\n",
        "\n",
        "\n",
        "            for window in windows:\n",
        "              self.data.append(window)\n",
        "\n",
        "              if self.unified_label:\n",
        "                self.labels.append(f\"{label_dict['fault_location']}_{label_dict['crack_size']}\" if label_dict['normal'] == False else \"normal\")\n",
        "              else:\n",
        "                self.labels.append(label_dict)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        window = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            window = self.transform(window).astype('float32')\n",
        "\n",
        "        return window, label\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkVWvbwf6hfU"
      },
      "source": [
        "The dataset class above seeks to make the most of the data available in the bearings dataset by splitting each sample in the file into multiple overlapping windows. This increases the effective number of training samples and helps models learn more robust patterns. However, care must be taken to avoid data leakage between training and test sets when using overlapping windows - if we were to pull all the data and then split into train/test, windows from the same original sample could end up in both sets.\n",
        "\n",
        "To prevent this, we ensure that all windows derived from a given file are assigned to either the training or test set exclusively by splitting into train/test at the file level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2etv3hUJ6hfV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "all_files = list(Path(\"classification-cwru-mat-organized\").rglob(\"*.mat\"))\n",
        "\n",
        "# derive one label per file\n",
        "file_labels = [\n",
        "    '_'.join(f.parent.parts[-2:])\n",
        "    for f in all_files\n",
        "]\n",
        "\n",
        "train_files, test_files = train_test_split(\n",
        "    all_files,\n",
        "    test_size=.2,\n",
        "    shuffle=True,\n",
        "    stratify=file_labels\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rekM3G0s6hfV"
      },
      "source": [
        "##\n",
        "\n",
        "We want to set up a class for testing different classification techniques on the bearings dataset. The class will accept a dataset object and classification model, and be able to train and evaluate the model consistently for metrics like accuracy, precision, recall, and F1-score as well as time for training and inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "6Zq-Isg86hfV"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class ClassificationTrainTestEvaluate(ABC):\n",
        "    def __init__(self, train_dataset: Dataset, test_dataset: Dataset):\n",
        "        self.train_dataset = train_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "\n",
        "        self.time_metrics = {\n",
        "            \"train_time\": None,\n",
        "            \"inference_time\": None\n",
        "        }\n",
        "\n",
        "        self.model = None\n",
        "\n",
        "    def load_model(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def classification_report(self):\n",
        "        \"\"\"\n",
        "        Creates a Plotly subplot with:\n",
        "        - Confusion matrix heatmap\n",
        "        - Metrics summary table\n",
        "\n",
        "        Parameters:\n",
        "        - test_y : array-like of true labels\n",
        "        - test_prediction : array-like of predicted labels\n",
        "        - class_names : optional list of class label names\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute confusion matrix\n",
        "        cm = confusion_matrix(self.test_y, self.predictions)\n",
        "\n",
        "        # Compute metrics (handles binary & multiclass)\n",
        "        accuracy = accuracy_score(self.test_y, self.predictions)\n",
        "        precision = precision_score(self.test_y, self.predictions, average='weighted', zero_division=0)\n",
        "        recall = recall_score(self.test_y, self.predictions, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(self.test_y, self.predictions, average='weighted', zero_division=0)\n",
        "\n",
        "        # Create subplot layout\n",
        "        fig = make_subplots(\n",
        "            rows=1, cols=2,\n",
        "            column_widths=[0.6, 0.4],\n",
        "            specs=[[{\"type\": \"heatmap\"}, {\"type\": \"table\"}]],\n",
        "            subplot_titles=(\"Confusion Matrix\", \"Model Performance Metrics\")\n",
        "        )\n",
        "\n",
        "        # --- Confusion Matrix Heatmap ---\n",
        "        fig.add_trace(\n",
        "            go.Heatmap(\n",
        "                z=cm,\n",
        "                x=self.class_names,\n",
        "                y=self.class_names,\n",
        "                text=cm,\n",
        "                texttemplate=\"%{text}\",\n",
        "                colorscale=\"Blues\",\n",
        "                showscale=False\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        fig.update_xaxes(title_text=\"Predicted Label\", row=1, col=1)\n",
        "        fig.update_yaxes(title_text=\"True Label\", row=1, col=1)\n",
        "\n",
        "        # --- Metrics Table ---\n",
        "        fig.add_trace(\n",
        "            go.Table(\n",
        "                header=dict(values=[\"Metric\", \"Value\"],\n",
        "                            fill_color=\"lightgrey\",\n",
        "                            align=\"center\"),\n",
        "                cells=dict(values=[\n",
        "                    [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"],\n",
        "                    [f\"{accuracy:.4f}\", f\"{precision:.4f}\", f\"{recall:.4f}\", f\"{f1:.4f}\"]\n",
        "                ],\n",
        "                align=\"center\")\n",
        "            ),\n",
        "            row=1, col=2\n",
        "        )\n",
        "\n",
        "        fig.update_layout(\n",
        "            title=\"Model Evaluation Summary\",\n",
        "            height=500,\n",
        "            width=900\n",
        "        )\n",
        "\n",
        "        fig.show()\n",
        "\n",
        "class SciKitCTTE(ClassificationTrainTestEvaluate):\n",
        "    def prepare_data(self):\n",
        "        self.train_X, self.train_y = pd.DataFrame(), pd.Series()\n",
        "        print(\"Preparing training data...\")\n",
        "        for i in tqdm(range(len(self.train_dataset))):\n",
        "            X_chunk, label = self.train_dataset[i]\n",
        "\n",
        "            self.train_X = pd.concat([self.train_X, X_chunk], ignore_index=True)\n",
        "            self.train_y = pd.concat([self.train_y, pd.Series(label)], ignore_index=True)\n",
        "\n",
        "        self.test_X, self.test_y = pd.DataFrame(), pd.Series()\n",
        "        print(\"Preparing test data...\")\n",
        "        for i in tqdm(range(len(self.test_dataset))):\n",
        "            X_chunk, labels = self.test_dataset[i]\n",
        "\n",
        "            self.test_X = pd.concat([self.test_X, X_chunk], ignore_index=True)\n",
        "            self.test_y = pd.concat([self.test_y, pd.Series(labels)], ignore_index=True)\n",
        "\n",
        "    def train(self, train_X, train_y):\n",
        "        self.model.fit(train_X, train_y)\n",
        "        self.class_names = sorted(self.train_y.unique())\n",
        "\n",
        "    def evaluate(self, test_X, test_y):\n",
        "        self.predictions = self.model.predict(test_X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNl5Ed836hfV"
      },
      "source": [
        "# Feature Extraction + Feature Based Classification\n",
        "\n",
        "With a dataset abstraction in place, we can now explore different families of time-series classification techniques. The goal here is not only to compare performance, but also to understand how different representation choices affect model behavior on sensor-like signals.\n",
        "\n",
        "We begin with feature-based methods, which transform raw time-series into fixed-length statistical representations. These approaches are often strong baselines, easier to interpret, and computationally efficient compared to end-to-end deep learning models.\n",
        "\n",
        "### Feature Extraction\n",
        "\n",
        "We will implement a custom transformer class for the PyTorch dataset to extract statistical features using the `cesium` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "yZLiYT5C6hfV"
      },
      "outputs": [],
      "source": [
        "from cesium import featurize\n",
        "\n",
        "class FeatureExtractionTransform(Module):\n",
        "    def forward(self, window):\n",
        "        features_to_use = [\n",
        "            \"amplitude\",\n",
        "            \"percent_beyond_1_std\",\n",
        "            \"maximum\",\n",
        "            \"max_slope\",\n",
        "            \"median\",\n",
        "            \"median_absolute_deviation\",\n",
        "            \"percent_close_to_median\",\n",
        "            \"minimum\",\n",
        "            \"period_fast\",\n",
        "            \"skew\",\n",
        "            \"std\",\n",
        "        ]\n",
        "\n",
        "        fset = featurize.featurize_time_series(\n",
        "            times=np.arange(len(window)),\n",
        "            values=window,\n",
        "            errors=None,\n",
        "            features_to_use=features_to_use,\n",
        "        )\n",
        "\n",
        "        fset = fset.stack(future_stack=True)\n",
        "\n",
        "        return fset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ZHK6_RwQ6hfW"
      },
      "outputs": [],
      "source": [
        "train_dataset = BearingDataset(\n",
        "    train_files,\n",
        "    sampling_rate=SamplingRate.sr48K,\n",
        "    fault_location=FaultLocation.DE,\n",
        "    chunk_length=1200,\n",
        "    unified_label=True,\n",
        "    transform=FeatureExtractionTransform()\n",
        ")\n",
        "\n",
        "test_dataset = BearingDataset(\n",
        "    test_files,\n",
        "    sampling_rate=SamplingRate.sr48K,\n",
        "    fault_location=FaultLocation.DE,\n",
        "    chunk_length=1200,\n",
        "    unified_label=True,\n",
        "    transform=FeatureExtractionTransform()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "XZbEHq_n6hfW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b774b268-c305-48e4-8bb8-806f89f7c662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing training data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16551/16551 [07:03<00:00, 39.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4757/4757 [01:52<00:00, 42.24it/s]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "\n",
        "sk_ctte = SciKitCTTE(\n",
        "    train_dataset,\n",
        "    test_dataset)\n",
        "\n",
        "sk_ctte.prepare_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "sk_trainer = sk_ctte\n",
        "sk_trainer.load_model(rfc)\n",
        "\n",
        "sk_trainer.train(sk_trainer.train_X, sk_trainer.train_y)\n",
        "sk_trainer.evaluate(sk_trainer.test_X, sk_trainer.test_y)\n",
        "sk_trainer.classification_report()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "QyKA2bPqXwU1",
        "outputId": "d0814f1b-5dc6-478d-e285-8c772d883f5d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"88684c42-a08c-4b3a-a9fa-30a4b79cf865\" class=\"plotly-graph-div\" style=\"height:500px; width:900px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"88684c42-a08c-4b3a-a9fa-30a4b79cf865\")) {                    Plotly.newPlot(                        \"88684c42-a08c-4b3a-a9fa-30a4b79cf865\",                        [{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"showscale\":false,\"text\":[[394,31,47,0,2,0,0,336,1,0],[16,470,10,6,43,1,8,43,10,0],[1,1,394,4,23,19,1,11,52,0],[0,6,10,184,0,0,0,0,2,0],[1,11,0,0,189,54,0,47,0,0],[0,0,0,0,0,193,9,0,0,0],[0,0,0,0,0,0,501,0,6,0],[86,11,30,6,1,2,7,438,27,0],[55,7,0,4,10,0,26,12,495,0],[0,0,0,0,0,0,0,0,0,403]],\"texttemplate\":\"%{text}\",\"x\":[\"B_007\",\"B_014\",\"B_021\",\"IR_007\",\"IR_014\",\"IR_021\",\"OR_007\",\"OR_014\",\"OR_021\",\"normal_48k\"],\"y\":[\"B_007\",\"B_014\",\"B_021\",\"IR_007\",\"IR_014\",\"IR_021\",\"OR_007\",\"OR_014\",\"OR_021\",\"normal_48k\"],\"z\":[[394,31,47,0,2,0,0,336,1,0],[16,470,10,6,43,1,8,43,10,0],[1,1,394,4,23,19,1,11,52,0],[0,6,10,184,0,0,0,0,2,0],[1,11,0,0,189,54,0,47,0,0],[0,0,0,0,0,193,9,0,0,0],[0,0,0,0,0,0,501,0,6,0],[86,11,30,6,1,2,7,438,27,0],[55,7,0,4,10,0,26,12,495,0],[0,0,0,0,0,0,0,0,0,403]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"cells\":{\"align\":\"center\",\"values\":[[\"Accuracy\",\"Precision\",\"Recall\",\"F1 Score\"],[\"0.7696\",\"0.7835\",\"0.7696\",\"0.7687\"]]},\"header\":{\"align\":\"center\",\"fill\":{\"color\":\"lightgrey\"},\"values\":[\"Metric\",\"Value\"]},\"type\":\"table\",\"domain\":{\"x\":[0.64,1.0],\"y\":[0.0,1.0]}}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.54],\"title\":{\"text\":\"Predicted Label\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"True Label\"}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Confusion Matrix\",\"x\":0.27,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Model Performance Metrics\",\"x\":0.8200000000000001,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Model Evaluation Summary\"},\"height\":500,\"width\":900},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('88684c42-a08c-4b3a-a9fa-30a4b79cf865');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = SVC(kernel='linear', C=.1, random_state=42)\n",
        "\n",
        "sk_trainer = sk_ctte\n",
        "sk_trainer.load_model(svm)\n",
        "\n",
        "sk_trainer.train(sk_trainer.train_X, sk_trainer.train_y)\n",
        "sk_trainer.evaluate(sk_trainer.test_X, sk_trainer.test_y)\n",
        "sk_trainer.classification_report()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "6Wq0vPwDYPDv",
        "outputId": "4a20fba0-0921-412a-f100-bcad2af80f9a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"2c8e2609-f6e7-4f8b-98d9-f825190b8a62\" class=\"plotly-graph-div\" style=\"height:500px; width:900px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2c8e2609-f6e7-4f8b-98d9-f825190b8a62\")) {                    Plotly.newPlot(                        \"2c8e2609-f6e7-4f8b-98d9-f825190b8a62\",                        [{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"showscale\":false,\"text\":[[287,21,477,0,0,0,0,25,0,1],[62,310,33,59,83,10,0,8,2,40],[101,1,331,26,0,39,5,0,3,0],[142,58,0,0,0,0,0,2,0,0],[22,65,10,9,97,80,0,19,0,0],[0,0,0,1,0,125,72,0,4,0],[0,0,0,0,0,0,491,0,16,0],[175,13,322,32,6,60,0,0,0,0],[98,8,1,76,34,5,5,0,382,0],[0,1,0,0,0,0,0,0,0,402]],\"texttemplate\":\"%{text}\",\"x\":[\"B_007\",\"B_014\",\"B_021\",\"IR_007\",\"IR_014\",\"IR_021\",\"OR_007\",\"OR_014\",\"OR_021\",\"normal_48k\"],\"y\":[\"B_007\",\"B_014\",\"B_021\",\"IR_007\",\"IR_014\",\"IR_021\",\"OR_007\",\"OR_014\",\"OR_021\",\"normal_48k\"],\"z\":[[287,21,477,0,0,0,0,25,0,1],[62,310,33,59,83,10,0,8,2,40],[101,1,331,26,0,39,5,0,3,0],[142,58,0,0,0,0,0,2,0,0],[22,65,10,9,97,80,0,19,0,0],[0,0,0,1,0,125,72,0,4,0],[0,0,0,0,0,0,491,0,16,0],[175,13,322,32,6,60,0,0,0,0],[98,8,1,76,34,5,5,0,382,0],[0,1,0,0,0,0,0,0,0,402]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"cells\":{\"align\":\"center\",\"values\":[[\"Accuracy\",\"Precision\",\"Recall\",\"F1 Score\"],[\"0.5098\",\"0.5011\",\"0.5098\",\"0.4902\"]]},\"header\":{\"align\":\"center\",\"fill\":{\"color\":\"lightgrey\"},\"values\":[\"Metric\",\"Value\"]},\"type\":\"table\",\"domain\":{\"x\":[0.64,1.0],\"y\":[0.0,1.0]}}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.54],\"title\":{\"text\":\"Predicted Label\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"True Label\"}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Confusion Matrix\",\"x\":0.27,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Model Performance Metrics\",\"x\":0.8200000000000001,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Model Evaluation Summary\"},\"height\":500,\"width\":900},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2c8e2609-f6e7-4f8b-98d9-f825190b8a62');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6j9W0Qc6hfW"
      },
      "source": [
        "# Deep Learning Models\n",
        "\n",
        "In this section, I will explore a wide variety of neural network models to find which can perform the best at what is essentially a many-to-one problem, where we are giving the model a dataset of many measurements of vibrational movement where ordering matters, because those measurements unfolded across time.\n",
        "\n",
        "4.1 [PyTorch Trainer and Evaluator](##\n",
        "PyTorch-Trainer-and-Evaluator)\n",
        "\n",
        "4.2 [Fully Connected Neural Networks]()\n",
        "\n",
        "4.3 [Recurrent Neural Networks]()\n",
        "\n",
        "4.3.1 [Classic Recurrent Neural Network]()\n",
        "\n",
        "4.3.2 [Long Short Term Memory (LSTM) Neural Network]()\n",
        "\n",
        "4.3.3 [Gated Recurrent Neural Network]()\n",
        "\n",
        "4.4 [Convolutional Neural Networks]()\n",
        "\n",
        "4.4.1 [1D Convolutional Neural Network]()\n",
        "\n",
        "4.4.2 [Temporal Convolutional Network]()\n",
        "\n",
        "4.5 [Attention Based Models]()\n",
        "\n",
        "4.5.1 [LSTM with Attention]()\n",
        "\n",
        "4.5.2 [Time Series Transformer]()\n",
        "\n",
        "https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnJFPo3RWk0h",
        "outputId": "ebe14bd4-3746-4817-ec3e-33f166f6c5f4"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: False\n",
            "GPU count: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PyTorch Trainer and Evaluator"
      ],
      "metadata": {
        "id": "Aaqrd4QTsE8B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "qznbgXrq6hfW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch import Tensor, float32, LongTensor\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "class PyTorchCTTE(ClassificationTrainTestEvaluate):\n",
        "    def __init__(self, train_dataset: Dataset, test_dataset: Dataset, device='cpu', criterion=None, optimizer=None):\n",
        "        super().__init__(train_dataset, test_dataset)\n",
        "        self.device = device\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "\n",
        "        self.target_mapping = {\n",
        "          'fault_location': {'B': 0, 'IR': 1, 'OR': 2},\n",
        "          'crack_size': {'007': 0, '014': 1, '021': 2}\n",
        "          }\n",
        "\n",
        "        self.train_dataset_mean = None\n",
        "        self.train_dataset_std = None\n",
        "\n",
        "    def load_model(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def prepare_data(self):\n",
        "        self.train_dataloader = DataLoader(self.train_dataset, batch_size=64, shuffle=True)\n",
        "        self.test_dataloader = DataLoader(self.test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    def train(self, epochs: int, batch_size: int):\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        self.train_dataset_mean = np.mean(np.concatenate(self.train_dataset.data))\n",
        "        self.train_dataset_std = np.std(np.concatenate(self.train_dataset.data))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            epoch_loss = 0.0\n",
        "\n",
        "            progress_bar = tqdm(self.train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "            for batch_X, batch_y in progress_bar:\n",
        "                # X\n",
        "                batch_X = (Tensor(batch_X.to(float32)) - self.train_dataset_mean) / self.train_dataset_std\n",
        "                batch_X = batch_X.to(self.device)\n",
        "\n",
        "                # ys\n",
        "                batch_y_fault_detection = [int(l) for l in batch_y['normal']]\n",
        "                batch_y_fault_location = [self.target_mapping['fault_location'].get(l, -1) for l in batch_y['fault_location']]\n",
        "                batch_y_crack_size = [self.target_mapping['crack_size'].get(l, -1) for l in batch_y['crack_size']]\n",
        "                print(batch_y_fault_detection)\n",
        "                print(batch_y_fault_location)\n",
        "                print(batch_y_crack_size)\n",
        "\n",
        "                # move to GPU\n",
        "                batch_y_fault_detection = Tensor(batch_y_fault_detection).to(LongTensor).to(self.device)\n",
        "                batch_y_fault_location = Tensor(batch_y_fault_location).to(LongTensor).to(self.device)\n",
        "                batch_y_crack_size = Tensor(batch_y_crack_size).to(LongTensor).to(self.device)\n",
        "\n",
        "                # clear gradients before training\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # run the inputs through the network\n",
        "                fault_detection, fault_location, crack_size = self.model(batch_X)\n",
        "\n",
        "                # calculate the loss\n",
        "                loss_fault_detection = self.criterion(fault_detection, batch_y_fault_detection)\n",
        "                loss_fault_location = self.criterion(fault_location, batch_y_fault_location)\n",
        "                loss_crack_size = self.criterion(crack_size, batch_y_crack_size)\n",
        "\n",
        "                # sum to total loss\n",
        "                total_loss = loss_fault_detection + loss_fault_location + loss_crack_size\n",
        "\n",
        "                # backpropagate\n",
        "                total_loss.backward()\n",
        "\n",
        "                self.optimizer.step()\n",
        "\n",
        "                epoch_loss += total_loss.item()\n",
        "                progress_bar.set_postfix(loss=total_loss.item())\n",
        "\n",
        "            print(f\"Epoch {epoch+1} avg loss: {epoch_loss/len(self.train_dataloader):.4f}\")\n",
        "\n",
        "    #~ TODO this is likely not working\n",
        "    def evaluate(self):\n",
        "        self.test_y = [self.class_labels_dict[label] for label in self.test_dataloader.dataset.labels]\n",
        "        self.predictions = []\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in self.test_dataloader:\n",
        "                batch_X = (Tensor(batch_X.to(float32)) - self.train_dataset_mean) / self.train_dataset_std\n",
        "                batch_X = Tensor(batch_X.to(float32)).to(self.device)\n",
        "                outputs = self.model(batch_X)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                self.predictions.extend(predicted.cpu().numpy().tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Datasets for Deep Learning"
      ],
      "metadata": {
        "id": "0i5zcQ8IoQZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = BearingDataset(\n",
        "    train_files,\n",
        "    sampling_rate=SamplingRate.sr48K,\n",
        "    fault_location=FaultLocation.DE,\n",
        "    unified_label=False,\n",
        "    chunk_length=1200\n",
        ")\n",
        "\n",
        "test_dataset = BearingDataset(\n",
        "    test_files,\n",
        "    sampling_rate=SamplingRate.sr48K,\n",
        "    fault_location=FaultLocation.DE,\n",
        "    unified_label=False,\n",
        "    chunk_length=1200\n",
        ")"
      ],
      "metadata": {
        "id": "2Xy3Y3G6oTA7"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fully Connected Neural Network\n",
        "\n",
        "### Model Intuition\n",
        "\n",
        "\n",
        "### Model Definition"
      ],
      "metadata": {
        "id": "aatdzOAusiOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "class FCNN(nn.Module):\n",
        "    def __init__(self, input_dim=1200, num_fault_locations=3, num_crack_sizes=3):\n",
        "        super(FCNN, self).__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.fault_detection_output = nn.Linear(256, 1)\n",
        "        self.fault_location_output = nn.Linear(256, num_fault_locations)\n",
        "        self.crack_size_output = nn.Linear(256, num_crack_sizes)\n",
        "\n",
        "        # Xavier initialization\n",
        "        nn.init.xavier_uniform_(self.fault_detection_output.weight)\n",
        "        nn.init.xavier_uniform_(self.fault_location_output.weight)\n",
        "        nn.init.xavier_uniform_(self.crack_size_output.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.shared(x)\n",
        "\n",
        "        return (\n",
        "            torch.sigmoid(self.fault_detection_output(x)),\n",
        "            self.fault_location_output(x),\n",
        "            self.crack_size_output(x)\n",
        "        )"
      ],
      "metadata": {
        "id": "6xjrzpaqsdZO"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fcnn_model = FCNN(\n",
        "    input_dim=1200\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(fcnn_model.parameters(), lr=1e-3)\n",
        "\n",
        "fcnn_ctte = PyTorchCTTE(\n",
        "    train_dataset,\n",
        "    test_dataset,\n",
        "    device=device,\n",
        "    criterion=CrossEntropyLoss(),\n",
        "    optimizer=optimizer\n",
        ")\n",
        "\n",
        "fcnn_ctte.load_model(fcnn_model)"
      ],
      "metadata": {
        "id": "bzTD3su0oKQ3"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "cHeN777EqEW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fcnn_ctte.prepare_data()\n",
        "fcnn_ctte.train(epochs=25, batch_size=64)\n",
        "fcnn_ctte.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "3Kqr201ko55J",
        "outputId": "4a68b74d-7d56-44c5-c8a6-28789b5facc7"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/25:   0%|          | 0/259 [00:00<?, ?it/s]/tmp/ipython-input-2294538176.py:54: UserWarning:\n",
            "\n",
            "Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at /pytorch/torch/csrc/utils/python_arg_parser.cpp:342.)\n",
            "\n",
            "Epoch 1/25:   0%|          | 0/259 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[2, 1, 0, -1, 0, 1, 1, 0, -1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 0, 0, 2, 1, -1, 1, 1, 1, 2, 2, 1, 2, 0, 2, 1, 2, 2, 0, 2, 0, 0, 0, 2, 1, 2, 0, -1, 1, -1, 1, 2, 0, 0, 1, 1, 1, 0, -1, 1, 2, 2, 2, 2, 1, 2]\n",
            "[0, 1, 1, -1, 0, 2, 0, 1, -1, 2, 2, 0, 1, 2, 2, 2, 0, 0, 1, 2, 2, 1, 1, -1, 1, 1, 1, 0, 1, 0, 2, 2, 2, 0, 1, 2, 2, 2, 0, 0, 2, 0, 0, 0, 1, -1, 0, -1, 2, 0, 0, 0, 2, 2, 0, 2, -1, 2, 2, 0, 1, 0, 1, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "to() received an invalid combination of arguments - got (torch.tensortype), but expected one of:\n * (torch.device device = None, torch.dtype dtype = None, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)\n * (torch.dtype dtype, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)\n * (Tensor tensor, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-398365100.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfcnn_ctte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfcnn_ctte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfcnn_ctte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2294538176.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;31m# move to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mbatch_y_fault_detection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y_fault_detection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0mbatch_y_fault_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y_fault_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mbatch_y_crack_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y_crack_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1653\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1654\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1655\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_default_nowrap_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: to() received an invalid combination of arguments - got (torch.tensortype), but expected one of:\n * (torch.device device = None, torch.dtype dtype = None, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)\n * (torch.dtype dtype, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)\n * (Tensor tensor, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification Report"
      ],
      "metadata": {
        "id": "_df6kHjbp-Io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fcnn_ctte.classification_report()"
      ],
      "metadata": {
        "id": "8FiqW9iap3kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet\n",
        "\n",
        "https://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "\n",
        "### Model Intuition\n",
        "\n",
        "\n",
        "\n",
        "### Model Definition"
      ],
      "metadata": {
        "id": "32zHz6NsqoIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock1D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, downsample=False):\n",
        "        super(ResidualBlock1D, self).__init__()\n",
        "        self.downsample = downsample\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=1, stride=2 if downsample else 1)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if downsample or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=2 if downsample else 1),\n",
        "                nn.BatchNorm1d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += identity\n",
        "        return F.relu(out)\n",
        "\n",
        "class ResNet1D_pt(nn.Module):\n",
        "    def __init__(self, input_size=1024, num_classes=4):\n",
        "        super(ResNet1D_pt, self).__init__()\n",
        "\n",
        "        self.layer1 = ResidualBlock1D(1, 32)\n",
        "        self.layer2 = ResidualBlock1D(32, 64, downsample=True)\n",
        "        self.layer3 = ResidualBlock1D(64, 128, downsample=True)\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc1 = nn.Linear(128, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "\n",
        "        # Four separate outputs\n",
        "        self.fc_check = nn.Linear(128, 1)          # Binary classification\n",
        "        self.fc_type = nn.Linear(128, num_classes) # Fault type classification\n",
        "        self.fc_size_cls = nn.Linear(128, num_classes) # Fault size classification (multi-class)\n",
        "        self.fc_size_reg = nn.Linear(128, 1)       # Fault size regression\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.pool(x).squeeze(-1)  # Global Avg Pooling\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        out_check = torch.sigmoid(self.fc_check(x))  # Binary classification\n",
        "        out_type = self.fc_type(x)                   # Multi-class classification\n",
        "        out_size_cls = self.fc_size_cls(x)           # Fault size as classification\n",
        "        out_size_reg = torch.clamp(self.fc_size_reg(x), min=0)  # Fault size regression (non-negative)\n",
        "\n",
        "        return out_check, out_type, out_size_cls, out_size_reg"
      ],
      "metadata": {
        "id": "-sPFfqDAq8P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Preparation"
      ],
      "metadata": {
        "id": "krZIsQJdqqeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "lFEDs3FWqvzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification Report"
      ],
      "metadata": {
        "id": "K8kYSbSmqxTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Long Short Term Memory (LSTM) Neural Network\n",
        "\n",
        "### Model Intuition\n",
        "\n",
        "LSTMs are a subtype of recurrent neural networks.\n",
        "\n",
        "https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n",
        "### Model Definition"
      ],
      "metadata": {
        "id": "Tn81_irNsNJZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U07xDcGo6hfW"
      },
      "outputs": [],
      "source": [
        "class LSTM1D_pt(nn.Module):\n",
        "    def __init__(self, sequence_length=1024, hidden_size=128, num_layers=2, dropout_rate=0.3, num_classes=4):\n",
        "        super(LSTM1D_pt, self).__init__()\n",
        "        self.sequence_length = sequence_length\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=1,\n",
        "                            hidden_size=hidden_size,\n",
        "                            num_layers=num_layers,\n",
        "                            batch_first=True,\n",
        "                            dropout=dropout_rate,\n",
        "                            bidirectional=False)\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(hidden_size)\n",
        "\n",
        "        # Single output head for multi-class classification\n",
        "        self.fc_type = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for layer in [self.fc_type]:\n",
        "            nn.init.xavier_uniform_(layer.weight)\n",
        "            if layer.bias is not None:\n",
        "                nn.init.zeros_(layer.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1)\n",
        "        lstm_out, (h_n, _) = self.lstm(x)\n",
        "\n",
        "        # Instead of only last hidden state:\n",
        "        features = lstm_out.mean(dim=1)\n",
        "        features = self.bn(features) # Apply BatchNorm1d\n",
        "\n",
        "        out_type = self.fc_type(features)\n",
        "        return out_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qvTHho56hfW"
      },
      "outputs": [],
      "source": [
        "train_dataset = BearingDataset(\n",
        "    train_files,\n",
        "    sampling_rate=SamplingRate.sr48K,\n",
        "    fault_location=FaultLocation.DE,\n",
        "    chunk_length=1200\n",
        ")\n",
        "\n",
        "test_dataset = BearingDataset(\n",
        "    test_files,\n",
        "    sampling_rate=SamplingRate.sr48K,\n",
        "    fault_location=FaultLocation.DE,\n",
        "    chunk_length=1200\n",
        ")\n",
        "\n",
        "lstm_model = LSTM1D_pt(\n",
        "    sequence_length=1200,\n",
        "    hidden_size=256,\n",
        "    num_layers=2,\n",
        "    dropout_rate=0.1,\n",
        "    num_classes=len(set(train_dataset.labels))\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=1e-3)\n",
        "\n",
        "lstm_ctte = PyTorchCTTE(\n",
        "    train_dataset,\n",
        "    test_dataset,\n",
        "    model=lstm_model,\n",
        "    device=device,\n",
        "    criterion=CrossEntropyLoss(),\n",
        "    optimizer=optimizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut3rlFxx6hfW"
      },
      "outputs": [],
      "source": [
        "lstm_ctte.prepare_data()\n",
        "lstm_ctte.train(epochs=25, batch_size=64)\n",
        "lstm_ctte.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juPVfhZY6hfW"
      },
      "outputs": [],
      "source": [
        "lstm_ctte.classification_report()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXH_Wzn06hfW"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}