{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Classification\n",
    "\n",
    "This notebook explores various time series classification techniques. It makes much fuller use of the bearings dataset also explored in the signal analysis notebook.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Time series classification involves assigning time series instances to predefined categories. This notebook will cover:\n",
    "\n",
    "- **Feature-Based Methods**: Statistical features, tsfresh, Catch22\n",
    "- **Distance-Based Methods**: DTW-KNN, Euclidean Distance\n",
    "- **Dictionary-Based Methods**: BOSS, cBOSS\n",
    "- **Shapelet-Based Methods**: Shapelet Transform\n",
    "- **Deep Learning Methods**: CNN, ResNet, InceptionTime\n",
    "- **Ensemble Methods**: HIVE-COTE, TS-CHIEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Here I am implementing a PyTorch dataset for the bearings dataset. While we will explore various classification techniques, the dataset loading and preprocessing will remain consistent including for simpler feature-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.nn import Module\n",
    "import scipy.io\n",
    "import enum\n",
    "\n",
    "# samplng rate enum\n",
    "class SamplingRate(enum.Enum):\n",
    "    sr12K = \"12k\"\n",
    "    sr48K = \"48k\"\n",
    "\n",
    "class FaultLocation(enum.Enum):\n",
    "    DE = \"drive_end_fault\"\n",
    "    FE = \"front_end_fault\"\n",
    "\n",
    "\n",
    "class BearingDataset(Dataset):\n",
    "    def __init__(self, file_paths, sampling_rate, fault_location, chunk_length, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.fault_location = fault_location\n",
    "        self.chunk_length = chunk_length\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "\n",
    "        mat_data = scipy.io.loadmat(file_path)\n",
    "\n",
    "        key_to_match = f\"_{str(self.fault_location)[-2:]}_time\"\n",
    "        sensor_key = [key for key in mat_data.keys() if key_to_match in key][0]\n",
    "\n",
    "        signal = mat_data[sensor_key].squeeze()\n",
    "\n",
    "        n_chunks = len(signal) // self.chunk_length\n",
    "        truncated = signal[:n_chunks * self.chunk_length]\n",
    "\n",
    "        windows = truncated.reshape(n_chunks, self.chunk_length)\n",
    "\n",
    "        label = '_'.join(file_path.parent.parts[-2:])\n",
    "        labels = [label, ] * n_chunks\n",
    "\n",
    "        if self.transform:\n",
    "            windows, label = self.transform(windows, label)\n",
    "\n",
    "        # return ALL windows for this file\n",
    "        return windows, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Techniques\n",
    "\n",
    "### Feature-Based Methods\n",
    "\n",
    "The first set of techniques focuses on extracting meaningful features from time series data to facilitate classification using traditional classification algorithms. We will implement a custom transforer class for the PyTorch dataset to extract statistical features using the `cesium` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cesium import featurize\n",
    "\n",
    "class FeatureExtractionTransform(Module):\n",
    "    def forward(self, stacked_chunks, label):\n",
    "        features_to_use = [\n",
    "            \"amplitude\",\n",
    "            \"percent_beyond_1_std\",\n",
    "            \"maximum\",\n",
    "            \"max_slope\",\n",
    "            \"median\",\n",
    "            \"median_absolute_deviation\",\n",
    "            \"percent_close_to_median\",\n",
    "            \"minimum\",\n",
    "            \"skew\",\n",
    "            \"std\",\n",
    "        ]\n",
    "\n",
    "        fset = featurize.featurize_time_series(\n",
    "            times=np.arange(stacked_chunks.shape[1]),\n",
    "            values=stacked_chunks,\n",
    "            errors=None,\n",
    "            features_to_use=features_to_use,\n",
    "        )\n",
    "\n",
    "        fset = fset.stack(future_stack=True)\n",
    "\n",
    "        return fset, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "all_files = list(Path(\"../data/classification/cwru-bearing-full-organized\").rglob(\"*.mat\"))\n",
    "\n",
    "# derive one label per file\n",
    "file_labels = [\n",
    "    '_'.join(f.parent.parts[-2:])\n",
    "    for f in all_files\n",
    "]\n",
    "\n",
    "train_files, test_files = train_test_split(\n",
    "    all_files,\n",
    "    test_size=.2,\n",
    "    shuffle=True,\n",
    "    stratify=file_labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BearingDataset(\n",
    "    train_files,\n",
    "    sampling_rate=SamplingRate.sr48K,\n",
    "    fault_location=FaultLocation.DE,\n",
    "    chunk_length=48000,\n",
    "    transform=FeatureExtractionTransform()\n",
    ")\n",
    "\n",
    "test_dataset = BearingDataset(\n",
    "    test_files,\n",
    "    sampling_rate=SamplingRate.sr48K,\n",
    "    fault_location=FaultLocation.DE,\n",
    "    chunk_length=48000,\n",
    "    transform=FeatureExtractionTransform()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "0 of 89\n",
      "1 of 89\n",
      "2 of 89\n",
      "3 of 89\n",
      "4 of 89\n",
      "5 of 89\n",
      "6 of 89\n",
      "7 of 89\n",
      "8 of 89\n",
      "9 of 89\n",
      "10 of 89\n",
      "11 of 89\n",
      "12 of 89\n",
      "13 of 89\n",
      "14 of 89\n",
      "15 of 89\n",
      "16 of 89\n",
      "17 of 89\n",
      "18 of 89\n",
      "19 of 89\n",
      "20 of 89\n",
      "21 of 89\n",
      "22 of 89\n",
      "23 of 89\n",
      "24 of 89\n",
      "25 of 89\n",
      "26 of 89\n",
      "27 of 89\n",
      "28 of 89\n",
      "29 of 89\n",
      "30 of 89\n",
      "31 of 89\n",
      "32 of 89\n",
      "33 of 89\n",
      "34 of 89\n",
      "35 of 89\n",
      "36 of 89\n",
      "37 of 89\n",
      "38 of 89\n",
      "39 of 89\n",
      "40 of 89\n",
      "41 of 89\n",
      "42 of 89\n",
      "43 of 89\n",
      "44 of 89\n",
      "45 of 89\n",
      "46 of 89\n",
      "47 of 89\n",
      "48 of 89\n",
      "49 of 89\n",
      "50 of 89\n",
      "51 of 89\n",
      "52 of 89\n",
      "53 of 89\n",
      "54 of 89\n",
      "55 of 89\n",
      "56 of 89\n",
      "57 of 89\n",
      "58 of 89\n",
      "59 of 89\n",
      "60 of 89\n",
      "61 of 89\n",
      "62 of 89\n",
      "63 of 89\n",
      "64 of 89\n",
      "65 of 89\n",
      "66 of 89\n",
      "67 of 89\n",
      "68 of 89\n",
      "69 of 89\n",
      "70 of 89\n",
      "71 of 89\n",
      "72 of 89\n",
      "73 of 89\n",
      "74 of 89\n",
      "75 of 89\n",
      "76 of 89\n",
      "77 of 89\n",
      "78 of 89\n",
      "79 of 89\n",
      "80 of 89\n",
      "81 of 89\n",
      "82 of 89\n",
      "83 of 89\n",
      "84 of 89\n",
      "85 of 89\n",
      "86 of 89\n",
      "87 of 89\n",
      "88 of 89\n",
      "Training data shape: (387, 10)\n",
      "Test data shape: (103, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing training data...\")\n",
    "train_X, train_y = pd.DataFrame(), pd.Series()\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    print(i, \"of\", len(train_dataset))\n",
    "    X_chunk, labels = train_dataset[i]\n",
    "\n",
    "    train_X = pd.concat([train_X, X_chunk], ignore_index=True)\n",
    "    train_y = pd.concat([train_y, pd.Series(labels)], ignore_index=True)\n",
    "\n",
    "print(\"Training data shape:\", train_X.shape)\n",
    "\n",
    "test_X, test_y = pd.DataFrame(), pd.Series()\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    X_chunk, labels = test_dataset[i]\n",
    "\n",
    "    test_X = pd.concat([test_X, X_chunk], ignore_index=True)\n",
    "    test_y = pd.concat([test_y, pd.Series(labels)], ignore_index=True)\n",
    "\n",
    "print(\"Test data shape:\", test_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier = rfc.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B_007       0.87      0.87      0.87        15\n",
      "       B_014       0.83      0.83      0.83         6\n",
      "       B_021       0.31      0.83      0.45         6\n",
      "      IR_007       0.80      1.00      0.89        12\n",
      "      IR_014       0.67      0.17      0.27        12\n",
      "      IR_021       1.00      1.00      1.00        15\n",
      "      OR_007       0.85      0.92      0.88        12\n",
      "      OR_014       0.40      0.33      0.36         6\n",
      "      OR_021       1.00      0.71      0.83        14\n",
      "  normal_48k       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.78       103\n",
      "   macro avg       0.77      0.77      0.74       103\n",
      "weighted avg       0.82      0.78      0.77       103\n",
      "\n",
      "Confusion Matrix:\n",
      " [[13  0  0  0  0  0  0  2  0  0]\n",
      " [ 0  5  0  0  1  0  0  0  0  0]\n",
      " [ 0  1  5  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 12  0  0  0  0  0  0]\n",
      " [ 0  0  9  1  2  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 15  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 11  1  0  0]\n",
      " [ 1  0  0  2  0  0  1  2  0  0]\n",
      " [ 1  0  2  0  0  0  1  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "test_predictions = rf_classifier.predict(test_X)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(test_y, test_predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(test_y, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09819227, 0.07980937, 0.08927998, 0.09511949, 0.04562255,\n",
       "       0.15653409, 0.06582102, 0.07662608, 0.05438362, 0.1500449 ,\n",
       "       0.08856665])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance-Based Methods\n",
    "\n",
    "Techniques to be implemented:\n",
    "- Dynamic Time Warping (DTW) with KNN\n",
    "- Euclidean Distance KNN\n",
    "- Edit Distance on Real Sequences (EDR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance-based methods will be implemented here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary-Based Methods\n",
    "\n",
    "Techniques to be implemented:\n",
    "- BOSS (Bag of SFA Symbols)\n",
    "- cBOSS (Contractable BOSS)\n",
    "- WEASEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary-based methods will be implemented here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapelet-Based Methods\n",
    "\n",
    "Techniques to be implemented:\n",
    "- Shapelet Transform\n",
    "- Learning Shapelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapelet-based methods will be implemented here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Methods\n",
    "\n",
    "Techniques to be implemented:\n",
    "- Convolutional Neural Networks (CNN)\n",
    "- ResNet for Time Series\n",
    "- InceptionTime\n",
    "- LSTM/GRU Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning methods will be implemented here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods\n",
    "\n",
    "Techniques to be implemented:\n",
    "- HIVE-COTE (Hierarchical Vote Collective of Transformation-based Ensembles)\n",
    "- TS-CHIEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble methods will be implemented here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Compare the performance of different classification techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison will be implemented here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Summary of findings and best practices for time series classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts-trove",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
